{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Lesson 1\n",
    "    Worked on in connection with O'Reilly NLP course w.safaribooksonline.com/videos/natural-language-processing\n",
    "    \n",
    "    Below code reproduced manually from here https://github.com/bmtgoncalves/FromScratch/blob/master/NLP/NLP%20Lesson%201.ipynb\n",
    "    with my own additions/subtractions/comments\n",
    "\"\"\"\n",
    "\n",
    "import string\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "text = \"\"\"Mary had a little lamb, little lamb,\n",
    "    little lamb. Mary had a little lamb\n",
    "    whose fleece was white as snow.\n",
    "    And everywhere that Mary went\n",
    "    Mary went, Mary went. Everywhere\n",
    "    that Mary went,\n",
    "    The lamb was sure to go\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize words from the text\n",
    "def extract_words(text):\n",
    "    temp = text.split()\n",
    "    text_words = []\n",
    "    \n",
    "    # Get rid of punctuation and downcase each word\n",
    "    for word in temp:\n",
    "        while word[0] in string.punctuation:\n",
    "            word = word[1:]\n",
    "            \n",
    "        while word[-1] in string.punctuation:\n",
    "            word = word[:-1]\n",
    "        \n",
    "        text_words.append(word.lower())\n",
    "    \n",
    "    return text_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_words = extract_words(text)\n",
    "\n",
    "# key: word, value: unique word_id\n",
    "word_dict = {}\n",
    "# list of unique words from text_words\n",
    "word_list = []\n",
    "# list of word_ids in order as words appear in text_words\n",
    "text_tokens = []\n",
    "# counter to generate word_ids\n",
    "word_id = 0\n",
    "\n",
    "\"\"\"\n",
    "    take each tokenized word and 1) add it to the\n",
    "    word_dict and unique word_list if not already\n",
    "    there (i.e., no word repeats) and 2) append \n",
    "    it's word_id to the text_tokens list\n",
    "    (i.e., repeats ok)\n",
    "\"\"\"\n",
    "for word in text_words:\n",
    "    if word not in word_dict:\n",
    "        word_dict[word] = word_id\n",
    "        word_list.append(word)\n",
    "        word_id += 1\n",
    "    \n",
    "    text_tokens.append(word_dict[word])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mary': 0, 'had': 1, 'a': 2, 'little': 3, 'lamb': 4, 'whose': 5, 'fleece': 6, 'was': 7, 'white': 8, 'as': 9, 'snow': 10, 'and': 11, 'everywhere': 12, 'that': 13, 'went': 14, 'the': 15, 'sure': 16, 'to': 17, 'go': 18}\n",
      "[0, 1, 2, 3, 4, 3, 4, 3, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0, 14, 0, 14, 0, 14, 12, 13, 0, 14, 15, 4, 7, 16, 17, 18]\n"
     ]
    }
   ],
   "source": [
    "print(word_dict)\n",
    "\n",
    "# tokenized version of text\n",
    "print(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "    ONE_HOT ENCODING\n",
    "    This isn't super useful. We don't know enough about the words \n",
    "    when we represent them with a single int.\n",
    "    Use one-hot encoding instead. Generate a unique vector for each word,\n",
    "    where 1 appears once at an index in the vector unique to that word.\n",
    "\"\"\"\n",
    "\n",
    "def one_hot(word, word_dict=word_dict):\n",
    "    # generate an np vector of all zeros\n",
    "    vector = np.zeros(len(word_dict))\n",
    "    # flip 0 to 1 at the index where the word's word_id is\n",
    "    vector[word_dict[word]] = 1\n",
    "    return vector\n",
    "\n",
    "# one_hot definitions of 'mary' and 'fleece'\n",
    "print(one_hot('mary'))\n",
    "print(one_hot('fleece'))\n",
    "print(one_hot('mary') + one_hot('fleece'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5ba4295444db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \"\"\"\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtext_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabulary_size' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "    BAG OF WORDS\n",
    "    One-hot vectorization works ok above because relatively small\n",
    "    dictionary of words. If we were attempting to one-hot vectorize\n",
    "    the entire english language, we'd have a vector space of a million\n",
    "    which wouldn't be efficient. We'd have a vector with lenght a million\n",
    "    or so for each word, and only one value would be flipped to 1. A waste\n",
    "    of zeros, basically.\n",
    "    \n",
    "    Bag of words: Just keep track of words you are using in text. Use a\n",
    "    dictionary with key word and value how many times word appears. You lose\n",
    "    the word order but get rid of all the vector wasted 0s.\n",
    "\"\"\"\n",
    "\n",
    "text_vector = np.zeros(vocabulary_size)\n",
    "\n",
    "for word in text_words:\n",
    "    # count no. of times word appears by adding\n",
    "    # 1 to vector position at word_index\n",
    "    text_vector[word_dict[word]] += 1\n",
    "    \n",
    "print(text_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'mary' is at word_index 0. Looks like she appears 6 times.\n",
    "# Confirm: How many times does 'mary' appear?\n",
    "print(text_vector[word_dict['mary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same thin use the python Counter module\n",
    "word_counts = Counter(text_words)\n",
    "pprint(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    STOP WORDS\n",
    "    Words that carry little meaning and can be discarded\n",
    "    \"the\", \"and\", \"a\", etc\n",
    "    Often occur frequently, but not always so can't just throw\n",
    "    words out based on frequency\n",
    "    Usually use manually curated list of words created by linguists\n",
    "\"\"\"\n",
    "\n",
    "# get list of items [(key, value),...] from word_count above and sort by frequency descending\n",
    "items = list(word_counts.items())\n",
    "sorted(items, key=lambda x:x[1], reverse=True)\n",
    "\n",
    "# Not easy to draw conclusions from above small piece of text\n",
    "# import the first 100 MB of the english Wikipedia from: http://mattmahoney.net/dc/textdata\n",
    "\n",
    "data = []\n",
    "\n",
    "for line in gzip.open(\"data/text8.gz\", 'rt'):\n",
    "    data.extend(line.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count and sort the wikipedia data\n",
    "counts = Counter(data)\n",
    "sorted_counts = sorted(list(counts.items()), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "# print the top 10 works. Note that they aren't particularly interesting or useful.\n",
    "for word, count in sorted_counts[:10]:\n",
    "    print(word, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot word frequency distribution\n",
    "# Note that a lot of words are used infrequently (upper left)\n",
    "# and a fiew words are used very frequency (lower right)\n",
    "# and everything else clumps in the middle\n",
    "dist = Counter(counts.values())\n",
    "dist = list(dist.items())\n",
    "dist.sort(key=lambda x:x[0])\n",
    "dist = np.array(dist)\n",
    "\n",
    "norm = np.dot(dist.T[0], dist.T[1])\n",
    "\n",
    "plt.loglog(dist.T[0], dist.T[1]/norm)\n",
    "plt.xlabel(\"count\")\n",
    "plt.ylabel(\"P(count)\")\n",
    "plt.title(\"Word frequency distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's trying setting the top 100 most frequently used words\n",
    "# as stop words and removing them. In real life we would use\n",
    "# a curated list of words, but this will do for the example.\n",
    "\n",
    "stopwords = set([word for word, count in sorted_counts[:100]])\n",
    "clean_data = []\n",
    "\n",
    "for word in data:\n",
    "    if word not in stopwords:\n",
    "        clean_data.append(word)\n",
    "\n",
    "# we get rid of a lot of words--almost half\n",
    "print(\"Original size:\", len(data))\n",
    "print(\"Clean size:\", len(clean_data))\n",
    "print(\"Reduction %:\", (1-len(clean_data)/len(data)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mary', 'had', 'a', 'little', 'lamb', 'little', 'lamb', 'little', 'lamb'], ['mary', 'had', 'a', 'little', 'lamb', 'whose', 'fleece', 'was', 'white', 'as', 'snow'], ['and', 'everywhere', 'that', 'mary', 'went', 'mary', 'went', 'mary', 'went'], ['everywhere', 'that', 'mary', 'went', 'the', 'lamb', 'was', 'sure', 'to', 'go']]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    TF/IDF\n",
    "    \n",
    "    TF: Term Frequency (number of times word appears).\n",
    "        After removal of stop words (not meaningufl),\n",
    "        TF is a good indicator of what words are\n",
    "        important/meaningful in a document\n",
    "    IDF: Inverse Document Frequency (how unusual it is for a document\n",
    "        to contain a certain word). This let's you compare a word ACROSS\n",
    "        documents, the idea being the more often a word appears across\n",
    "        documents, the less meaningful it is to a specific document\n",
    "    \n",
    "    TF and IDF are sort of inverses. In TF, the most frequent word is most\n",
    "    meaningful to a doc. In IDF, the least frequent word is most \n",
    "    meaningful to a doc b/c it let's you distinguist it from other docs.\n",
    "    \n",
    "    TF and IDF each have different ways you can calculate them mathematically,\n",
    "    but not matter which you use TF-IDF is TF * IDF. High TF = important to \n",
    "    specific doc, and HIGH IDF (aka LOW DF) equals uncommon in entire corpus of docs.\n",
    "    \n",
    "    TF-IDF gives you a balance of term frequency and inverse doc frequency.\n",
    "    \n",
    "    One way to find stop words would be to look for words with high TF and low IDF.\n",
    "\"\"\"\n",
    "\n",
    "# start with our nursery rhyme again. Split it into four sentences.\n",
    "# Pretend each sentence is its own document.\n",
    "corpus_text = text.split('.')\n",
    "corpus_words = []\n",
    "\n",
    "for document in corpus_text:\n",
    "    doc_words = extract_words(document)\n",
    "    corpus_words.append(doc_words)\n",
    "    \n",
    "print(corpus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of documents in which each word appears\n",
    "document_count = {}\n",
    "\n",
    "for document in corpus_words:\n",
    "    word_set = set(document)\n",
    "    \n",
    "    for word in word_set:\n",
    "        document_count[word] = document_count.get(word, 0) + 1\n",
    "\n",
    "pprint(document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'mary' appears in all 4 documents. So, it's pretty useless to use to\n",
    "# distinguish between the documents.\n",
    "pprint(document_count['mary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'go' appears in just 1 document, that's good!\n",
    "pprint(document_count['go'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now calculate IDF\n",
    "# as an aside, I don't know why this guy keeps\n",
    "# passing these globals in as arguments, going to make\n",
    "# them defaults\n",
    "def inv_doc_freq(corpus_words=corpus_words):\n",
    "    number_docs = len(corpus_words)\n",
    "    document_count = {}\n",
    "    \n",
    "    for document in corpus_words:\n",
    "        word_set = set(document)\n",
    "        \n",
    "        for word in word_set:\n",
    "            document_count[word] = document_count.get(word, 0) + 1\n",
    "    \n",
    "    IDF = {}\n",
    "    \n",
    "    for word in document_count:\n",
    "        # log of total docs / number of docs word appears in\n",
    "        # use log to avoid handling small fractional numbers.\n",
    "        IDF[word] = np.log(number_docs/document_count[word])\n",
    "        \n",
    "    return IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0.6931471805599453,\n",
      " 'and': 1.3862943611198906,\n",
      " 'as': 1.3862943611198906,\n",
      " 'everywhere': 0.6931471805599453,\n",
      " 'fleece': 1.3862943611198906,\n",
      " 'go': 1.3862943611198906,\n",
      " 'had': 0.6931471805599453,\n",
      " 'lamb': 0.28768207245178085,\n",
      " 'little': 0.6931471805599453,\n",
      " 'mary': 0.0,\n",
      " 'snow': 1.3862943611198906,\n",
      " 'sure': 1.3862943611198906,\n",
      " 'that': 0.6931471805599453,\n",
      " 'the': 1.3862943611198906,\n",
      " 'to': 1.3862943611198906,\n",
      " 'was': 0.6931471805599453,\n",
      " 'went': 0.6931471805599453,\n",
      " 'white': 1.3862943611198906,\n",
      " 'whose': 1.3862943611198906}\n"
     ]
    }
   ],
   "source": [
    "# note that IDF gives a smaller weight to most common words\n",
    "# From the guy teaching this:\n",
    "# \"As expected Mary has the smallest weight of all words 0, \n",
    "# meaning that it is effectively removed from the dataset.\n",
    "# You can consider this as a way of implicitly identify and remove\n",
    "# stopwords. In case you do want to keep even the words that appear\n",
    "# in every document, you can just add a 1. to the argument of the\n",
    "# logarithm above\"\n",
    "pprint(inv_doc_freq())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'mary': 0.0,\n",
       "          'had': 0.6931471805599453,\n",
       "          'a': 0.6931471805599453,\n",
       "          'little': 2.0794415416798357,\n",
       "          'lamb': 0.8630462173553426}),\n",
       " Counter({'mary': 0.0,\n",
       "          'had': 0.6931471805599453,\n",
       "          'a': 0.6931471805599453,\n",
       "          'little': 0.6931471805599453,\n",
       "          'lamb': 0.28768207245178085,\n",
       "          'whose': 1.3862943611198906,\n",
       "          'fleece': 1.3862943611198906,\n",
       "          'was': 0.6931471805599453,\n",
       "          'white': 1.3862943611198906,\n",
       "          'as': 1.3862943611198906,\n",
       "          'snow': 1.3862943611198906}),\n",
       " Counter({'and': 1.3862943611198906,\n",
       "          'everywhere': 0.6931471805599453,\n",
       "          'that': 0.6931471805599453,\n",
       "          'mary': 0.0,\n",
       "          'went': 2.0794415416798357}),\n",
       " Counter({'everywhere': 0.6931471805599453,\n",
       "          'that': 0.6931471805599453,\n",
       "          'mary': 0.0,\n",
       "          'went': 0.6931471805599453,\n",
       "          'the': 1.3862943611198906,\n",
       "          'lamb': 0.28768207245178085,\n",
       "          'was': 0.6931471805599453,\n",
       "          'sure': 1.3862943611198906,\n",
       "          'to': 1.3862943611198906,\n",
       "          'go': 1.3862943611198906})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply TF by IDF to see how relevant a word is to a doc\n",
    "\n",
    "def tf_idf(corpus_words):\n",
    "    IDF = inv_doc_freq(corpus_words)\n",
    "    \n",
    "    TFIDF = []\n",
    "    \n",
    "    for document in corpus_words:\n",
    "        TFIDF.append(Counter(document))\n",
    "    \n",
    "    for document in TFIDF:\n",
    "        for word in document:\n",
    "            document[word] = document[word]*IDF[word]\n",
    "            \n",
    "    return TFIDF\n",
    "\n",
    "tf_idf(corpus_words)\n",
    "\n",
    "# Paraphrasing: now have a vector representation of each of our documents.\n",
    "# Each vector is a unique representation of each document in the\n",
    "# corpus making it posssible to define the similarity of two documents, etc.\n",
    "\n",
    "# 'mary' has a weight of 0 as expected because it appears in every document.\n",
    "# 'little' has the strongest wait in the first document, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    STEMMING\n",
    "    \n",
    "    Finding the root, or stem, of a word to identify similar words and further reduce\n",
    "    number of words we need to consider (i.e., treat similar words as one word)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
